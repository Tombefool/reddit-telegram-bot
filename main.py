"""
Reddit Telegram è‡ªåŠ¨æ¨é€æœºå™¨äººä¸»ç¨‹åº
æ¯å¤©åŒ—äº¬æ—¶é—´ 09:00 è‡ªåŠ¨æŠ“å– Reddit è´¢ç»æ¿å—çƒ­é—¨å¸–å­å¹¶æ¨é€åˆ° Telegram
"""

import os
import sys
from datetime import datetime
from datetime import timedelta
import sqlite3
from dotenv import load_dotenv

from reddit_fetcher import fetch_multiple_subreddits
from summarizer import summarize_post, format_summary_for_telegram
from social_fetcher import fetch_youtube_rss, fetch_nitter_rss
from truth_social_fetcher import fetch_truth_social
from truth_social_playwright import fetch_truth_social_playwright
from telegram_sender import send_message_with_retry, format_message_for_telegram, validate_telegram_config


def load_configuration():
    """åŠ è½½ç¯å¢ƒé…ç½®"""
    load_dotenv()
    
    config = {
        'telegram_bot_token': os.getenv('TELEGRAM_BOT_TOKEN'),
        'chat_id': os.getenv('CHAT_ID'),
        'gemini_api_key': os.getenv('GEMINI_API_KEY'),
        'dry_run': os.getenv('DRY_RUN', '0') == '1',
        'filter_keywords': os.getenv('FILTER_KEYWORDS', ''),
    }
    
    # éªŒè¯å¿…éœ€é…ç½®
    if not config['telegram_bot_token'] and not config['dry_run']:
        print("âŒ é”™è¯¯: æœªè®¾ç½® TELEGRAM_BOT_TOKEN")
        sys.exit(1)
    
    if not config['chat_id'] and not config['dry_run']:
        print("âŒ é”™è¯¯: æœªè®¾ç½® CHAT_ID")
        sys.exit(1)
    
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    return config


def get_target_subreddits():
    """è·å–ç›®æ ‡ Reddit æ¿å—åˆ—è¡¨"""
    return [
        'stocks',
        'wallstreetbets', 
        'investing',
        'cryptocurrency',
        'bitcoin',
        # ä¸­ç¾å…³ç³»ç›¸å…³æ¿å—
        'China',
        'Sino',
        'China_News',
        'geopolitics',
        'worldnews',
        'politics',
        'news'
    ]


def process_posts(posts, gemini_api_key=None):
    """å¤„ç†å¸–å­,ç”Ÿæˆæ‘˜è¦"""
    processed_posts = []
    
    for post in posts:
        print(f"å¤„ç†å¸–å­: {post['title'][:50]}...")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = summarize_post(
            title=post['title'],
            text=post['selftext'],
            api_key=gemini_api_key
        )
        
        # æ ¼å¼åŒ–æ‘˜è¦
        formatted_summary = format_summary_for_telegram(summary)
        
        # æ·»åŠ åˆ°å¤„ç†åçš„å¸–å­
        processed_post = post.copy()
        processed_post['summary'] = formatted_summary
        processed_posts.append(processed_post)
    
    return processed_posts


def get_beijing_timestamp():
    """è·å–åŒ—äº¬æ—¶é—´æˆ³"""
    # GitHub Actions è¿è¡Œåœ¨ UTC æ—¶é—´
    # åŒ—äº¬æ—¶é—´ = UTC + 8
    utc_now = datetime.utcnow()
    beijing_time = utc_now + timedelta(hours=8)
    
    return beijing_time.strftime("%Y-%m-%d %H:%M")


def ensure_cache_table(conn):
    """ç¡®ä¿å»é‡ç¼“å­˜è¡¨å­˜åœ¨"""
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS pushed_posts (
            url TEXT PRIMARY KEY,
            pushed_at_utc INTEGER
        )
        """
    )
    conn.commit()


def is_recent(created_utc: float, freshness_hours: int = 2) -> bool:
    """åˆ¤æ–­å¸–å­æ˜¯å¦åœ¨æŒ‡å®šå°æ—¶å†…æ–°é²œ"""
    try:
        post_time = datetime.utcfromtimestamp(created_utc)
        return (datetime.utcnow() - post_time) <= timedelta(hours=freshness_hours)
    except Exception:
        return False


def filter_fresh_posts(posts, freshness_hours: int = 6):
    """æ™ºèƒ½è¿‡æ»¤æ–°é²œå¸–å­ï¼Œæ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´æ–°é²œåº¦è¦æ±‚"""
    current_time = datetime.now()
    fresh_posts = []
    
    for post in posts:
        try:
            created_time = datetime.fromtimestamp(post.get('created_utc', 0))
            hours_old = (current_time - created_time).total_seconds() / 3600
            
            # æ™ºèƒ½æ–°é²œåº¦åˆ¤æ–­
            if hours_old <= 2:
                post['freshness_score'] = 3  # éå¸¸æ–°é²œ
            elif hours_old <= 6:
                post['freshness_score'] = 2  # æ¯”è¾ƒæ–°é²œ
            elif hours_old <= 24:
                post['freshness_score'] = 1  # ä¸€èˆ¬æ–°é²œ
            else:
                post['freshness_score'] = 0  # è¿‡æ—¶
            
            # æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´æ–°é²œåº¦è¦æ±‚
            title = post.get('title', '').lower()
            if any(keyword in title for keyword in ['breaking', 'urgent', 'live', 'just in']):
                # çªå‘æ–°é—»æ”¾å®½åˆ°12å°æ—¶
                if hours_old <= 12:
                    fresh_posts.append(post)
            elif any(keyword in title for keyword in ['analysis', 'opinion', 'review']):
                # åˆ†æç±»æ–‡ç« æ”¾å®½åˆ°48å°æ—¶
                if hours_old <= 48:
                    fresh_posts.append(post)
            elif hours_old <= freshness_hours:
                fresh_posts.append(post)
                
        except Exception as e:
            print(f"âš ï¸ æ—¶é—´è§£æé”™è¯¯: {e}")
            # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œé»˜è®¤åŒ…å«
            fresh_posts.append(post)
    
    return fresh_posts


def filter_by_keywords(posts, keyword_csv: str):
    """æŒ‰å…³é”®è¯è¿‡æ»¤ï¼Œä»…å¯¹ Truth Social/Nitter åº”ç”¨ï¼›ä¸ºç©ºä¸ç­›é€‰ã€‚è‹¥ç­›ç©ºåˆ™è‡ªåŠ¨æ”¾å®½ä¸ºâ€œä»…æ ‡é¢˜åŒ¹é…â€ã€‚"""
    kw = [k.strip().lower() for k in keyword_csv.split(',') if k.strip()]
    if not kw:
        return posts

    def is_social(p):
        src = (p.get('subreddit') or '').lower()
        return src in ('truth-social', 'trump-x')

    social, others = [], []
    for p in posts:
        (social if is_social(p) else others).append(p)

    # ä¸¥æ ¼ï¼šæ ‡é¢˜+æ­£æ–‡
    strict = []
    for p in social:
        text = (p.get('title','') + "\n" + p.get('selftext','')).lower()
        if any(k in text for k in kw):
            strict.append(p)

    if strict:
        return strict + others

    # æ”¾å®½ï¼šä»…æ ‡é¢˜
    relaxed = []
    for p in social:
        title = p.get('title','').lower()
        if any(k in title for k in kw):
            relaxed.append(p)

    return (relaxed if relaxed else social) + others


def filter_dedup(conn, posts, dedupe_hours: int = 24):
    """åŸºäº SQLite çš„å»é‡ï¼Œé»˜è®¤ 24 å°æ—¶å†…ç›¸åŒ URL ä¸é‡å¤æ¨é€"""
    now_ts = int(datetime.utcnow().timestamp())
    cutoff = now_ts - dedupe_hours * 3600

    # æ¸…ç†è¿‡æœŸè®°å½•
    conn.execute("DELETE FROM pushed_posts WHERE pushed_at_utc < ?", (cutoff,))
    conn.commit()

    results = []
    for p in posts:
        url = p.get('url')
        if not url:
            continue
        cur = conn.execute("SELECT 1 FROM pushed_posts WHERE url = ?", (url,))
        if cur.fetchone():
            continue
        results.append(p)
    return results


def calculate_content_score(post):
    """æ™ºèƒ½è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ†"""
    score = 0
    
    # æ¥æºæƒé‡è¯„åˆ†ï¼ˆæ›´å…¨é¢çš„æƒé‡ä½“ç³»ï¼‰
    source_weights = {
        'UN News': 8, 'NATO News': 8, 'EU News': 7,
        'BBC World': 7, 'Reuters': 7, 'South China Morning Post': 6,
        'Foreign Policy': 6, 'Al Jazeera': 5, 'CNN': 5,
        'Wall Street Journal': 6, 'Financial Times': 6,
        'truth-social': 4, 'trump-youtube': 3, 'reddit': 2
    }
    
    source = post.get('source', post.get('subreddit', '')).lower()
    for key, weight in source_weights.items():
        if key.lower() in source:
            score += weight
            break
    else:
        score += 1  # é»˜è®¤æƒé‡
    
    # æ–°é²œåº¦è¯„åˆ†ï¼ˆä½¿ç”¨ä¹‹å‰è®¡ç®—çš„åˆ†æ•°ï¼‰
    freshness_score = post.get('freshness_score', 0)
    score += freshness_score * 2  # æ–°é²œåº¦æƒé‡åŠ å€
    
    # å…³é”®è¯é‡è¦æ€§è¯„åˆ†ï¼ˆæ›´æ™ºèƒ½çš„å…³é”®è¯æ£€æµ‹ï¼‰
    title = post.get('title', '').lower()
    content = post.get('selftext', '').lower()
    text = f"{title} {content}"
    
    # é«˜ä¼˜å…ˆçº§å…³é”®è¯
    high_priority_keywords = [
        'breaking', 'urgent', 'crisis', 'emergency', 'alert',
        'war', 'conflict', 'attack', 'bomb', 'explosion',
        'election', 'vote', 'president', 'congress', 'senate',
        'market crash', 'recession', 'inflation', 'fed', 'interest rate'
    ]
    
    # ä¸­ä¼˜å…ˆçº§å…³é”®è¯
    medium_priority_keywords = [
        'analysis', 'report', 'study', 'research', 'data',
        'policy', 'law', 'regulation', 'trade', 'tariff',
        'technology', 'ai', 'artificial intelligence', 'cyber'
    ]
    
    # æ£€æŸ¥é«˜ä¼˜å…ˆçº§å…³é”®è¯
    for keyword in high_priority_keywords:
        if keyword in text:
            score += 4
            break
    
    # æ£€æŸ¥ä¸­ä¼˜å…ˆçº§å…³é”®è¯
    for keyword in medium_priority_keywords:
        if keyword in text:
            score += 2
            break
    
    # å†…å®¹è´¨é‡è¯„åˆ†
    content_length = len(post.get('selftext', ''))
    if content_length > 200:
        score += 2
    elif content_length > 100:
        score += 1
    
    # æ ‡é¢˜è´¨é‡è¯„åˆ†
    title_length = len(title)
    if 20 <= title_length <= 100:  # æ ‡é¢˜é•¿åº¦é€‚ä¸­
        score += 1
    
    # äº’åŠ¨åº¦è¯„åˆ†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    upvotes = post.get('ups', 0)
    comments = post.get('num_comments', 0)
    if upvotes > 100:
        score += 2
    elif upvotes > 50:
        score += 1
    
    if comments > 50:
        score += 1
    
    # é¿å…é‡å¤å†…å®¹è¯„åˆ†
    if 'repost' in text or 're:' in title.lower():
        score -= 2
    
    return max(0, score)  # ç¡®ä¿åˆ†æ•°ä¸ä¸ºè´Ÿ


def score_and_sort_posts(posts):
    """å¯¹å¸–å­è¿›è¡Œè¯„åˆ†å’Œæ’åº"""
    scored_posts = []
    for post in posts:
        score = calculate_content_score(post)
        post['quality_score'] = score
        scored_posts.append(post)
    
    # æŒ‰è´¨é‡è¯„åˆ†æ’åºï¼Œè¯„åˆ†ç›¸åŒæ—¶æŒ‰æ—¶é—´æ’åº
    return sorted(scored_posts, key=lambda x: (x['quality_score'], x.get('created_utc', 0)), reverse=True)


def smart_content_filter(posts):
    """æ™ºèƒ½å†…å®¹è¿‡æ»¤ï¼Œå»é™¤ä½è´¨é‡å’Œé‡å¤å†…å®¹"""
    if not posts:
        return posts
    
    filtered_posts = []
    seen_titles = set()
    seen_urls = set()
    
    for post in posts:
        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        quality_score = post.get('quality_score', 0)
        if quality_score < 3:  # è´¨é‡åˆ†æ•°å¤ªä½
            continue
        
        # æ ‡é¢˜å»é‡
        title = post.get('title', '').lower().strip()
        if not title or title in seen_titles:
            continue
        
        # URLå»é‡
        url = post.get('url', '')
        if url in seen_urls:
            continue
        
        # å†…å®¹é•¿åº¦æ£€æŸ¥
        content = post.get('selftext', '')
        if len(content) < 20 and quality_score < 8:  # å†…å®¹å¤ªçŸ­ä¸”è´¨é‡ä¸é«˜
            continue
        
        # æ ‡é¢˜è´¨é‡æ£€æŸ¥
        if len(title) < 10:  # æ ‡é¢˜å¤ªçŸ­
            continue
        
        # é¿å…æ˜æ˜¾çš„åƒåœ¾å†…å®¹
        spam_keywords = ['click here', 'free money', 'guaranteed', 'make money fast']
        if any(keyword in title for keyword in spam_keywords):
            continue
        
        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        filtered_posts.append(post)
        seen_titles.add(title)
        seen_urls.add(url)
    
    return filtered_posts


def mark_pushed(conn, posts):
    """å°†å·²æ¨é€çš„å¸–å­å†™å…¥å»é‡è¡¨"""
    now_ts = int(datetime.utcnow().timestamp())
    for p in posts:
        url = p.get('url')
        if not url:
            continue
        try:
            conn.execute(
                "INSERT OR REPLACE INTO pushed_posts(url, pushed_at_utc) VALUES(?, ?)",
                (url, now_ts),
            )
        except Exception:
            pass
    conn.commit()


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ Reddit Telegram Bot å¯åŠ¨")
    print("=" * 50)
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_configuration()
        
        # 2. éªŒè¯ Telegram é…ç½®
        if not config['dry_run']:
            print("ğŸ” éªŒè¯ Telegram é…ç½®...")
            if not validate_telegram_config(config['telegram_bot_token'], config['chat_id']):
                print("âŒ Telegram é…ç½®éªŒè¯å¤±è´¥")
                sys.exit(1)
        else:
            print("ğŸ§ª DRY_RUN æ¨¡å¼ï¼šè·³è¿‡ Telegram æ ¡éªŒä¸å‘é€")
        
        # 3. è·å–ç›®æ ‡æ¿å—
        subreddits = get_target_subreddits()
        print(f"ğŸ¯ ç›®æ ‡æ¿å—: {', '.join(subreddits)}")
        
        # 4. æŠ“å– Reddit å¸–å­ï¼ˆè‹¥å—é™å¯æš‚æ—¶è·³è¿‡ï¼Œä»…æŠ“å–ç¤¾äº¤æºï¼‰
        posts = []
        try:
            print("\nğŸ“¡ å¼€å§‹æŠ“å– Reddit å¸–å­...")
            reddit_posts = fetch_multiple_subreddits(
                subreddits,
                posts_per_subreddit=5,
                sort='new',
                time_period='day',
            )
            posts.extend(reddit_posts)
        except Exception:
            print("âš ï¸ Reddit æŠ“å–å¼‚å¸¸ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ¥æº")

        # 4.1 æŠ“å–ç¤¾äº¤å¹³å° (YouTube RSS ä¸ Nitter å¤‡é€‰)
        print("\nğŸ“º æŠ“å– YouTube é¢‘é“...")
        yt_channel = os.getenv('TRUMP_YT_CHANNEL_ID', '').strip()
        if not yt_channel:
            # é»˜è®¤ä½¿ç”¨ç‰¹æœ—æ™®å®˜æ–¹é¢‘é“ IDï¼Œå…é…ç½®å¯ç”¨
            yt_channel = 'UCp0hYYBW6IMayGgR-WeoCvQ'
            print("â„¹ï¸ æœªé…ç½® TRUMP_YT_CHANNEL_IDï¼Œå·²ä½¿ç”¨é»˜è®¤é¢‘é“ID")
        yt_posts = fetch_youtube_rss(channel_id=yt_channel, limit=5)
        if yt_posts:
            print(f"âœ… YouTube: {len(yt_posts)} æ¡")
            posts.extend(yt_posts)
        else:
            print("âš ï¸ YouTube æœªè·å–å†…å®¹")

        print("\nğŸ¦ æŠ“å– Nitter (X é•œåƒ)...")
        x_username = os.getenv('TRUMP_X_USERNAME', 'realDonaldTrump').strip()
        x_posts = fetch_nitter_rss(username=x_username, limit=5)
        if x_posts:
            print(f"âœ… Nitter: {len(x_posts)} æ¡")
            posts.extend(x_posts)
        else:
            print("âš ï¸ Nitter æœªè·å–å†…å®¹ (å¯èƒ½é™æµ/ç½‘ç»œé—®é¢˜)")

        # 4.2 æŠ“å–ä¸­ç¾å…³ç³»æ–°é—»
        print("\nğŸ‡ºğŸ‡¸ğŸ‡¨ğŸ‡³ æŠ“å–ä¸­ç¾å…³ç³»æ–°é—»...")
        try:
            from us_china_news_fetcher import fetch_us_china_news, filter_us_china_posts
            
            # æŠ“å–ä¸“é—¨çš„æ–°é—»æº
            us_china_news = fetch_us_china_news(max_items=3)
            if us_china_news:
                print(f"âœ… ä¸­ç¾å…³ç³»æ–°é—»: {len(us_china_news)} æ¡")
                posts.extend(us_china_news)
            
            # ä» Reddit å¸–å­ä¸­è¿‡æ»¤ä¸­ç¾å…³ç³»ç›¸å…³å†…å®¹
            us_china_reddit = filter_us_china_posts(posts)
            if us_china_reddit:
                print(f"âœ… Reddit ä¸­ç¾å…³ç³»å¸–å­: {len(us_china_reddit)} æ¡")
                # ç§»é™¤åŸå¸–å­ä¸­çš„ä¸­ç¾å…³ç³»å†…å®¹ï¼Œé¿å…é‡å¤
                posts = [p for p in posts if not p.get('category') == 'ä¸­ç¾å…³ç³»']
                posts.extend(us_china_reddit)
                
        except Exception as e:
            print(f"âš ï¸ ä¸­ç¾å…³ç³»æ–°é—»æŠ“å–å¤±è´¥: {e}")

        # 4.3 æŠ“å–å›½é™…å…³ç³»åŠ¨æ€
        print("\nğŸŒ æŠ“å–å›½é™…å…³ç³»åŠ¨æ€...")
        try:
            from international_relations_fetcher import fetch_international_organizations, fetch_conflict_news
            
            # æŠ“å–å›½é™…ç»„ç»‡åŠ¨æ€
            intl_org_news = fetch_international_organizations(max_items=2)
            if intl_org_news:
                print(f"âœ… å›½é™…ç»„ç»‡åŠ¨æ€: {len(intl_org_news)} æ¡")
                posts.extend(intl_org_news)
            
            # æŠ“å–åœ°åŒºå†²çªåŠ¨æ€
            conflict_news = fetch_conflict_news(max_items=2)
            if conflict_news:
                print(f"âœ… åœ°åŒºå†²çªåŠ¨æ€: {len(conflict_news)} æ¡")
                posts.extend(conflict_news)
                
        except Exception as e:
            print(f"âš ï¸ å›½é™…å…³ç³»åŠ¨æ€æŠ“å–å¤±è´¥: {e}")

        # 4.4 Truth Socialï¼ˆä¼˜å…ˆç¬¬ä¸‰æ–¹æ•°æ®é›†ï¼›æ— é…ç½®åˆ™ä½¿ç”¨ Playwright æŠ“å–ï¼‰
        print("\nğŸ“° æŠ“å– Truth Social...")
        ts_dataset = os.getenv('TRUTH_SOCIAL_DATASET_URL', '').strip()
        ts_token = os.getenv('APIFY_TOKEN', '').strip() or None
        if ts_dataset:
            ts_posts = fetch_truth_social(ts_dataset, limit=10, token=ts_token)
            if ts_posts:
                print(f"âœ… Truth Social: {len(ts_posts)} æ¡")
                posts.extend(ts_posts)
            else:
                print("âš ï¸ Truth Social æœªè·å–å†…å®¹")
        else:
            print("â„¹ï¸ æœªé…ç½®æ•°æ®é›†ï¼Œå°è¯•æœ¬åœ°æ— å¤´æŠ“å–ï¼ˆPlaywrightï¼‰...")
            ts_pw_posts = fetch_truth_social_playwright(username='realDonaldTrump', limit=10)
            if ts_pw_posts:
                print(f"âœ… Truth Social(Playwright): {len(ts_pw_posts)} æ¡")
                posts.extend(ts_pw_posts)
            else:
                print("âš ï¸ Truth Social(Playwright) æœªè·å–å†…å®¹ï¼ˆå·²å›é€€ç¼“å­˜ç­–ç•¥ï¼‰")
        
        # 4.3 æ™ºèƒ½æ–°é²œåº¦è¿‡æ»¤
        fresh_posts = filter_fresh_posts(posts, freshness_hours=6)
        print(f"ğŸ•’ æ™ºèƒ½æ–°é²œåº¦è¿‡æ»¤å: {len(fresh_posts)} ä¸ªå¸–å­")

        # 4.4 å…³é”®è¯è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        filtered_posts = filter_by_keywords(fresh_posts, config.get('filter_keywords',''))
        if config.get('filter_keywords'):
            print(f"ğŸ” å…³é”®è¯è¿‡æ»¤å: {len(filtered_posts)} ä¸ªï¼ˆå…³é”®è¯: {config.get('filter_keywords')}ï¼‰")
        else:
            filtered_posts = fresh_posts

        # 4.5 åŸºäº SQLite çš„å»é‡ï¼ˆ24 å°æ—¶ï¼‰ï¼›DRY_RUN ä¸‹è·³è¿‡å»é‡ï¼Œä¾¿äºæœ¬åœ°é¢„è§ˆ
        conn = sqlite3.connect('news_cache.db')
        ensure_cache_table(conn)
        if config['dry_run']:
            posts = filtered_posts
            print(f"â™»ï¸ DRY_RUN è·³è¿‡å»é‡: {len(posts)} ä¸ªå¸–å­")
        else:
            unique_posts = filter_dedup(conn, filtered_posts, dedupe_hours=24)
            print(f"â™»ï¸ å»é‡å: {len(unique_posts)} ä¸ªå¸–å­")
            posts = unique_posts

        # 4.6 æ™ºèƒ½å†…å®¹è´¨é‡è¯„åˆ†å’Œæ’åº
        posts = score_and_sort_posts(posts)
        print(f"ğŸ“Š æ™ºèƒ½å†…å®¹è´¨é‡è¯„åˆ†å®Œæˆï¼Œæœ€é«˜åˆ†: {posts[0]['quality_score'] if posts else 0}")
        
        # 4.7 æ™ºèƒ½å»é‡å’Œå†…å®¹è´¨é‡è¿‡æ»¤
        posts = smart_content_filter(posts)
        print(f"ğŸ§  æ™ºèƒ½å†…å®¹è¿‡æ»¤å: {len(posts)} ä¸ªå¸–å­")
        
        # 4.7 åŠ¨æ€æ¨é€æ•°é‡æ§åˆ¶
        base_limit = 15  # åŸºç¡€é™åˆ¶ä»10å¢åŠ åˆ°15
        time_of_day = datetime.now().hour
        if 6 <= time_of_day <= 12:  # æ—©ä¸Šæ¨é€æ›´å¤š
            push_limit = min(base_limit + 5, len(posts))
        elif 18 <= time_of_day <= 22:  # æ™šä¸Šæ­£å¸¸æ¨é€
            push_limit = min(base_limit, len(posts))
        else:  # å…¶ä»–æ—¶é—´é€‚ä¸­æ¨é€
            push_limit = min(base_limit + 2, len(posts))
        
        posts = posts[:push_limit]
        print(f"ğŸ“Š åŠ¨æ€æ¨é€é™åˆ¶: {push_limit} æ¡ï¼ˆåŸå§‹: {len(posts)} æ¡ï¼‰")

        # 4.7 å…œåº•ç­–ç•¥ï¼šå¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œå°è¯•ä»ç¼“å­˜è·å–
        if not posts:
            print("ğŸ”„ æ‰€æœ‰æŠ“å–æºéƒ½å¤±è´¥ï¼Œå°è¯•ä»ç¼“å­˜è·å–å†…å®¹...")
            try:
                from truth_social_playwright import load_truth_cache
                cached_posts = load_truth_cache(max_age_hours=48)
                if cached_posts:
                    print(f"ğŸ“¦ ä»ç¼“å­˜è·å–åˆ° {len(cached_posts)} ä¸ªå¸–å­")
                    posts = cached_posts[:5]  # é™åˆ¶ç¼“å­˜å†…å®¹æ•°é‡
                else:
                    print("âŒ ç¼“å­˜ä¹Ÿä¸ºç©ºï¼Œæ— æ³•è·å–ä»»ä½•å†…å®¹")
                    sys.exit(1)
            except Exception as e:
                print(f"âŒ ç¼“å­˜è·å–å¤±è´¥: {e}")
                sys.exit(1)
        
        print(f"âœ… æˆåŠŸè·å– {len(posts)} ä¸ªå¸–å­")
        
        # 5. å¤„ç†å¸–å­ (ç”Ÿæˆæ‘˜è¦)
        print("\nğŸ¤– å¼€å§‹å¤„ç†å¸–å­...")
        processed_posts = process_posts(posts, config['gemini_api_key'])
        
        # 6. æ ¼å¼åŒ–æ¶ˆæ¯
        print("\nğŸ“ æ ¼å¼åŒ–æ¶ˆæ¯...")
        timestamp = get_beijing_timestamp()
        message = format_message_for_telegram(processed_posts, timestamp)
        
        # 7. å‘é€åˆ° Telegram æˆ–æ‰“å°
        if config['dry_run']:
            print("\nğŸ“¤ DRY_RUNï¼šæ‰“å°æ¶ˆæ¯ï¼Œä¸å‘é€ Telegram")
            print("\n====== é¢„è§ˆå¼€å§‹ ======")
            print(message)
            print("====== é¢„è§ˆç»“æŸ ======\n")
            # DRY_RUN ä¸‹ä¸å†™å…¥å·²æ¨é€æ ‡è®°ï¼Œé¿å…å½±å“ä¸‹æ¬¡é¢„è§ˆ
            print("ğŸ‰ ä»»åŠ¡å®Œæˆ! DRY_RUN é¢„è§ˆæˆåŠŸ")
        else:
            print("\nğŸ“¤ å‘é€æ¶ˆæ¯åˆ° Telegram...")
            success = send_message_with_retry(
                config['telegram_bot_token'],
                config['chat_id'],
                message
            )
            
            if success:
                # 7.1 æ ‡è®°å·²æ¨é€ç”¨äºåç»­å»é‡
                try:
                    mark_pushed(conn, processed_posts)
                except Exception:
                    pass
                print("ğŸ‰ ä»»åŠ¡å®Œæˆ! æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ° Telegram")
            else:
                print("âŒ ä»»åŠ¡å¤±è´¥! æ¶ˆæ¯å‘é€å¤±è´¥")
                sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
