"""
ä¸­ç¾å…³ç³»æ–°é—»æŠ“å–æ¨¡å—
ä¸“é—¨æŠ“å–ä¸­ç¾å…³ç³»ç›¸å…³çš„æ–°é—»å’ŒåŠ¨æ€
"""

import requests
import feedparser
import time
from typing import List, Dict
from datetime import datetime, timedelta
import re


def fetch_us_china_news(max_items: int = 5) -> List[Dict]:
    """
    æŠ“å–ä¸­ç¾å…³ç³»ç›¸å…³æ–°é—»
    
    Args:
        max_items: æ¯ä¸ªæºæœ€å¤šæŠ“å–çš„æ–‡ç« æ•°é‡
    
    Returns:
        æ–°é—»æ–‡ç« åˆ—è¡¨
    """
    news_sources = [
        {
            "name": "South China Morning Post",
            "url": "https://www.scmp.com/rss/91/feed",
            "category": "ä¸­ç¾å…³ç³»"
        },
        {
            "name": "Foreign Policy",
            "url": "https://foreignpolicy.com/feed/",
            "category": "åœ°ç¼˜æ”¿æ²»"
        },
        {
            "name": "BBC World",
            "url": "http://feeds.bbci.co.uk/news/world/rss.xml",
            "category": "ä¸­ç¾å…³ç³»"
        },
        {
            "name": "CNN International",
            "url": "http://rss.cnn.com/rss/edition.rss",
            "category": "ä¸­ç¾å…³ç³»"
        },
        {
            "name": "Al Jazeera",
            "url": "https://www.aljazeera.com/xml/rss/all.xml",
            "category": "ä¸­ç¾å…³ç³»"
        }
    ]
    
    # ä¸­ç¾å…³ç³»å…³é”®è¯
    keywords = [
        "China", "US", "ä¸­ç¾", "è´¸æ˜“æˆ˜", "å…³ç¨", "åä¸º", "TikTok", "èŠ¯ç‰‡", "åŠå¯¼ä½“",
        "ä¸€å¸¦ä¸€è·¯", "Belt and Road", "å—æµ·", "South China Sea", "å°æ¹¾", "Taiwan",
        "é¦™æ¸¯", "Hong Kong", "æ–°ç–†", "Xinjiang", "äººæƒ", "human rights",
        "ç§‘æŠ€ç«äº‰", "tech competition", "ä¾›åº”é“¾", "supply chain", "è„±é’©", "decoupling",
        "æ‹œç™»", "Biden", "ç‰¹æœ—æ™®", "Trump", "ä¹ è¿‘å¹³", "Xi Jinping"
    ]
    
    all_news = []
    
    for source in news_sources:
        try:
            print(f"ğŸ“° æŠ“å– {source['name']}...")
            
            # è§£æ RSS æº
            feed = feedparser.parse(source['url'])
            
            if feed.bozo:
                print(f"âš ï¸ {source['name']} RSS è§£æå¤±è´¥")
                continue
            
            items_added = 0
            for entry in feed.entries[:max_items * 2]:  # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤
                if items_added >= max_items:
                    break
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­ç¾å…³ç³»å…³é”®è¯
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()
                content = f"{title} {summary}"
                
                # å…³é”®è¯åŒ¹é…
                if any(keyword.lower() in content for keyword in keywords):
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_time = datetime.now()
                    try:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_time = datetime(*entry.published_parsed[:6])
                    except:
                        pass
                    
                    # æ£€æŸ¥æ–°é²œåº¦ï¼ˆ24å°æ—¶å†…ï¼‰
                    if datetime.now() - pub_time <= timedelta(hours=24):
                        news_item = {
                            'title': entry.get('title', ''),
                            'url': entry.get('link', ''),
                            'selftext': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'summary': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'source': source['name'],
                            'category': source['category'],
                            'published_time': pub_time,
                            'created_utc': int(pub_time.timestamp()),
                            'subreddit': 'us-china-news',
                            'score': 0,
                            'num_comments': 0,
                            'author': source['name']
                        }
                        all_news.append(news_item)
                        items_added += 1
            
            print(f"âœ… {source['name']}: {items_added} æ¡ç›¸å…³æ–°é—»")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"âŒ æŠ“å– {source['name']} å¤±è´¥: {e}")
            continue
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    all_news.sort(key=lambda x: x['created_utc'], reverse=True)
    
    print(f"ğŸ“Š ä¸­ç¾å…³ç³»æ–°é—»æ€»è®¡: {len(all_news)} æ¡")
    return all_news


def filter_us_china_posts(posts: List[Dict]) -> List[Dict]:
    """
    ä» Reddit å¸–å­ä¸­è¿‡æ»¤å‡ºä¸­ç¾å…³ç³»ç›¸å…³å†…å®¹
    
    Args:
        posts: Reddit å¸–å­åˆ—è¡¨
    
    Returns:
        è¿‡æ»¤åçš„å¸–å­åˆ—è¡¨
    """
    keywords = [
        "china", "chinese", "us-china", "ä¸­ç¾", "è´¸æ˜“æˆ˜", "å…³ç¨", "åä¸º", "tiktok",
        "semiconductor", "chip", "taiwan", "hong kong", "xinjiang", "belt and road",
        "biden", "trump", "xi jinping", "trade war", "tech war", "decoupling"
    ]
    
    filtered_posts = []
    
    for post in posts:
        title = post.get('title', '').lower()
        content = post.get('selftext', '').lower()
        subreddit = post.get('subreddit', '').lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­ç¾å…³ç³»å…³é”®è¯
        if any(keyword in title or keyword in content for keyword in keywords):
            # æ ‡è®°ä¸ºä¸­ç¾å…³ç³»ç›¸å…³
            post['category'] = 'ä¸­ç¾å…³ç³»'
            post['subreddit'] = f"us-china-{post.get('subreddit', '')}"
            filtered_posts.append(post)
        # ç‰¹å®šæ¿å—ç›´æ¥åŒ…å«
        elif subreddit in ['china', 'sino', 'china_news', 'geopolitics']:
            post['category'] = 'ä¸­ç¾å…³ç³»'
            post['subreddit'] = f"us-china-{subreddit}"
            filtered_posts.append(post)
    
    return filtered_posts


if __name__ == "__main__":
    # æµ‹è¯•æŠ“å–
    print("ğŸ§ª æµ‹è¯•ä¸­ç¾å…³ç³»æ–°é—»æŠ“å–...")
    news = fetch_us_china_news(max_items=3)
    
    print(f"\nğŸ“° æŠ“å–ç»“æœ ({len(news)} æ¡):")
    for i, item in enumerate(news[:5], 1):
        print(f"\n{i}. {item['title']}")
        print(f"   æ¥æº: {item['source']}")
        print(f"   æ—¶é—´: {item['published_time'].strftime('%Y-%m-%d %H:%M')}")
        print(f"   é“¾æ¥: {item['url']}")
