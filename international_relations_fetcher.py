"""
å›½é™…ç»„ç»‡åŠ¨æ€æŠ“å–æ¨¡å—
ä¸“é—¨æŠ“å–è”åˆå›½ã€åŒ—çº¦ã€æ¬§ç›Ÿç­‰å›½é™…ç»„ç»‡çš„æœ€æ–°åŠ¨æ€
"""

import requests
import feedparser
import time
from typing import List, Dict
from datetime import datetime, timedelta


def fetch_international_organizations(max_items: int = 3) -> List[Dict]:
    """
    æŠ“å–å›½é™…ç»„ç»‡åŠ¨æ€
    
    Args:
        max_items: æ¯ä¸ªç»„ç»‡æœ€å¤šæŠ“å–çš„æ–‡ç« æ•°é‡
    
    Returns:
        å›½é™…ç»„ç»‡åŠ¨æ€åˆ—è¡¨
    """
    org_sources = [
        {
            "name": "BBC World",
            "url": "http://feeds.bbci.co.uk/news/world/rss.xml",
            "category": "å›½é™…ç»„ç»‡",
            "keywords": ["UN", "NATO", "EU", "WTO", "IMF", "G7", "G20", "international", "organization", "summit", "conference"]
        },
        {
            "name": "Reuters World",
            "url": "https://feeds.reuters.com/reuters/worldNews",
            "category": "å›½é™…ç»„ç»‡",
            "keywords": ["UN", "NATO", "EU", "WTO", "IMF", "G7", "G20", "international", "organization", "summit", "conference"]
        },
        {
            "name": "CNN International",
            "url": "http://rss.cnn.com/rss/edition.rss",
            "category": "å›½é™…ç»„ç»‡",
            "keywords": ["UN", "NATO", "EU", "WTO", "IMF", "G7", "G20", "international", "organization", "summit", "conference"]
        },
        {
            "name": "Al Jazeera",
            "url": "https://www.aljazeera.com/xml/rss/all.xml",
            "category": "å›½é™…ç»„ç»‡",
            "keywords": ["UN", "NATO", "EU", "WTO", "IMF", "G7", "G20", "international", "organization", "summit", "conference"]
        },
        {
            "name": "Deutsche Welle",
            "url": "https://rss.dw.com/rss/rss-en-all",
            "category": "å›½é™…ç»„ç»‡",
            "keywords": ["UN", "NATO", "EU", "WTO", "IMF", "G7", "G20", "international", "organization", "summit", "conference"]
        }
    ]
    
    all_news = []
    
    for source in org_sources:
        try:
            print(f"ğŸ›ï¸ æŠ“å– {source['name']}...")
            
            # è§£æ RSS æº
            feed = feedparser.parse(source['url'])
            
            if feed.bozo:
                print(f"âš ï¸ {source['name']} RSS è§£æå¤±è´¥")
                continue
            
            items_added = 0
            for entry in feed.entries[:max_items * 2]:
                if items_added >= max_items:
                    break
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()
                content = f"{title} {summary}"
                
                # å…³é”®è¯åŒ¹é…
                if any(keyword.lower() in content for keyword in source['keywords']):
                    # è§£æå‘å¸ƒæ—¶é—´
                    pub_time = datetime.now()
                    try:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_time = datetime(*entry.published_parsed[:6])
                    except:
                        pass
                    
                    # æ£€æŸ¥æ–°é²œåº¦ï¼ˆ48å°æ—¶å†…ï¼Œå›½é™…ç»„ç»‡åŠ¨æ€ç›¸å¯¹ç¨³å®šï¼‰
                    if datetime.now() - pub_time <= timedelta(hours=48):
                        news_item = {
                            'title': entry.get('title', ''),
                            'url': entry.get('link', ''),
                            'selftext': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'summary': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'source': source['name'],
                            'category': source['category'],
                            'published_time': pub_time,
                            'created_utc': int(pub_time.timestamp()),
                            'subreddit': f"intl-org-{source['category'].lower()}",
                            'score': 0,
                            'num_comments': 0,
                            'author': source['name']
                        }
                        all_news.append(news_item)
                        items_added += 1
            
            print(f"âœ… {source['name']}: {items_added} æ¡åŠ¨æ€")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"âŒ æŠ“å– {source['name']} å¤±è´¥: {e}")
            continue
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    all_news.sort(key=lambda x: x['created_utc'], reverse=True)
    
    print(f"ğŸ›ï¸ å›½é™…ç»„ç»‡åŠ¨æ€æ€»è®¡: {len(all_news)} æ¡")
    return all_news


def fetch_conflict_news(max_items: int = 3) -> List[Dict]:
    """
    æŠ“å–åœ°åŒºå†²çªä¸å®‰å…¨åŠ¨æ€
    
    Args:
        max_items: æ¯ä¸ªå†²çªåœ°åŒºæœ€å¤šæŠ“å–çš„æ–‡ç« æ•°é‡
    
    Returns:
        å†²çªåŠ¨æ€åˆ—è¡¨
    """
    conflict_sources = [
        {
            "name": "BBC World",
            "url": "http://feeds.bbci.co.uk/news/world/rss.xml",
            "category": "å…¨çƒå†²çª",
            "keywords": ["Ukraine", "Russia", "Israel", "Palestine", "Middle East", "conflict", "war"]
        },
        {
            "name": "Reuters World",
            "url": "https://feeds.reuters.com/reuters/worldNews",
            "category": "ä¸–ç•Œæ–°é—»",
            "keywords": ["conflict", "military", "security", "defense", "war", "peace"]
        },
        {
            "name": "Al Jazeera",
            "url": "https://www.aljazeera.com/xml/rss/all.xml",
            "category": "ä¸­ä¸œè§†è§’",
            "keywords": ["Middle East", "Palestine", "Israel", "Syria", "Iran", "Yemen"]
        }
    ]
    
    conflict_keywords = [
        "Ukraine", "Russia", "NATO", "war", "conflict", "military", "defense",
        "Israel", "Palestine", "Middle East", "Syria", "Iran", "Yemen",
        "North Korea", "DPRK", "nuclear", "missile", "sanctions",
        "Taiwan", "South China Sea", "territorial", "dispute"
    ]
    
    all_news = []
    
    for source in conflict_sources:
        try:
            print(f"âš”ï¸ æŠ“å– {source['name']} å†²çªåŠ¨æ€...")
            
            feed = feedparser.parse(source['url'])
            
            if feed.bozo:
                print(f"âš ï¸ {source['name']} RSS è§£æå¤±è´¥")
                continue
            
            items_added = 0
            for entry in feed.entries[:max_items * 3]:  # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤
                if items_added >= max_items:
                    break
                
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()
                content = f"{title} {summary}"
                
                # å†²çªå…³é”®è¯åŒ¹é…
                if any(keyword.lower() in content for keyword in conflict_keywords):
                    pub_time = datetime.now()
                    try:
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_time = datetime(*entry.published_parsed[:6])
                    except:
                        pass
                    
                    # æ£€æŸ¥æ–°é²œåº¦ï¼ˆ24å°æ—¶å†…ï¼Œå†²çªåŠ¨æ€æ—¶æ•ˆæ€§å¼ºï¼‰
                    if datetime.now() - pub_time <= timedelta(hours=24):
                        news_item = {
                            'title': entry.get('title', ''),
                            'url': entry.get('link', ''),
                            'selftext': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'summary': entry.get('summary', '')[:200] + '...' if len(entry.get('summary', '')) > 200 else entry.get('summary', ''),
                            'source': source['name'],
                            'category': 'åœ°åŒºå†²çª',
                            'published_time': pub_time,
                            'created_utc': int(pub_time.timestamp()),
                            'subreddit': 'conflict-security',
                            'score': 0,
                            'num_comments': 0,
                            'author': source['name']
                        }
                        all_news.append(news_item)
                        items_added += 1
            
            print(f"âœ… {source['name']}: {items_added} æ¡å†²çªåŠ¨æ€")
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ æŠ“å– {source['name']} å¤±è´¥: {e}")
            continue
    
    all_news.sort(key=lambda x: x['created_utc'], reverse=True)
    print(f"âš”ï¸ åœ°åŒºå†²çªåŠ¨æ€æ€»è®¡: {len(all_news)} æ¡")
    return all_news


if __name__ == "__main__":
    # æµ‹è¯•æŠ“å–
    print("ğŸ§ª æµ‹è¯•å›½é™…ç»„ç»‡åŠ¨æ€æŠ“å–...")
    org_news = fetch_international_organizations(max_items=2)
    
    print(f"\nğŸ›ï¸ å›½é™…ç»„ç»‡åŠ¨æ€ ({len(org_news)} æ¡):")
    for i, item in enumerate(org_news[:3], 1):
        print(f"\n{i}. {item['title']}")
        print(f"   ç»„ç»‡: {item['source']}")
        print(f"   æ—¶é—´: {item['published_time'].strftime('%Y-%m-%d %H:%M')}")
        print(f"   é“¾æ¥: {item['url']}")
    
    print("\n" + "="*50)
    
    print("ğŸ§ª æµ‹è¯•åœ°åŒºå†²çªåŠ¨æ€æŠ“å–...")
    conflict_news = fetch_conflict_news(max_items=2)
    
    print(f"\nâš”ï¸ åœ°åŒºå†²çªåŠ¨æ€ ({len(conflict_news)} æ¡):")
    for i, item in enumerate(conflict_news[:3], 1):
        print(f"\n{i}. {item['title']}")
        print(f"   æ¥æº: {item['source']}")
        print(f"   æ—¶é—´: {item['published_time'].strftime('%Y-%m-%d %H:%M')}")
        print(f"   é“¾æ¥: {item['url']}")
