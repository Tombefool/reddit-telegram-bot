#!/usr/bin/env python3
"""
ç»¼åˆç‰ˆæ–°é—» Telegram Bot - åŒ…å«ä¸­ç¾å…³ç³»ä¸“é¢˜
"""

import os
import sys
import requests
import xml.etree.ElementTree as ET
import time
import random
import json
import re
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, Any, List
from dotenv import load_dotenv
import google.generativeai as genai

load_dotenv()

def log(message):
    """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

class SourceHealthMonitor:
    """æºå¥åº·ç›‘æ§å™¨"""
    def __init__(self, db_path="sources_health.db"):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS source_health (
                source_name TEXT PRIMARY KEY,
                last_success TIMESTAMP,
                last_failure TIMESTAMP,
                failure_count INTEGER DEFAULT 0,
                is_healthy INTEGER DEFAULT 1,
                weight REAL DEFAULT 1.0
            )
        ''')
        conn.commit()
        conn.close()
    
    def record_success(self, source_name: str, weight: float = 1.0):
        """è®°å½•æˆåŠŸ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO source_health 
            (source_name, last_success, failure_count, is_healthy, weight)
            VALUES (?, ?, 0, 1, ?)
        ''', (source_name, datetime.now(), weight))
        conn.commit()
        conn.close()
    
    def record_failure(self, source_name: str):
        """è®°å½•å¤±è´¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO source_health 
            (source_name, last_failure, failure_count, is_healthy)
            VALUES (?, ?, COALESCE((SELECT failure_count FROM source_health WHERE source_name = ?), 0) + 1, 0)
        ''', (source_name, datetime.now(), source_name))
        conn.commit()
        conn.close()
    
    def is_healthy(self, source_name: str) -> bool:
        """æ£€æŸ¥æºæ˜¯å¦å¥åº·"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT is_healthy FROM source_health WHERE source_name = ?', (source_name,))
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else True
    
    def get_weight(self, source_name: str) -> float:
        """è·å–æºæƒé‡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT weight FROM source_health WHERE source_name = ?', (source_name,))
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else 1.0

class NewsCache:
    """æ–°é—»ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self, db_path="news_cache.db"):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                summary TEXT,
                url TEXT,
                source TEXT,
                category TEXT,
                weight REAL,
                timestamp TIMESTAMP,
                UNIQUE(title, source)
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_timestamp ON news_cache(timestamp)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_category ON news_cache(category)
        ''')
        conn.commit()
        conn.close()
    
    def add_news(self, news_item: Dict[str, Any], category: str, weight: float = 1.0):
        """æ·»åŠ æ–°é—»åˆ°ç¼“å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO news_cache 
                (title, summary, url, source, category, weight, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                news_item.get('title', ''),
                news_item.get('summary', ''),
                news_item.get('url', ''),
                news_item.get('source', ''),
                category,
                weight,
                datetime.now()
            ))
            conn.commit()
        except Exception as e:
            log(f"ç¼“å­˜æ·»åŠ å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def get_cached_news(self, category: str, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„æ–°é—»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT title, summary, url, source, weight, timestamp
            FROM news_cache 
            WHERE category = ? 
            ORDER BY weight DESC, timestamp DESC 
            LIMIT ?
        ''', (category, limit))
        
        results = []
        for row in cursor.fetchall():
            results.append({
                'title': row[0],
                'summary': row[1],
                'url': row[2],
                'source': row[3],
                'weight': row[4],
                'timestamp': row[5]
            })
        
        conn.close()
        return results
    
    def cleanup_old_news(self, hours: int = 24):
        """æ¸…ç†æ—§æ–°é—»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cutoff_time = datetime.now() - timedelta(hours=hours)
        cursor.execute('DELETE FROM news_cache WHERE timestamp < ?', (cutoff_time,))
        conn.commit()
        conn.close()

def get_beijing_timestamp():
    """è·å–åŒ—äº¬æ—¶é—´æˆ³"""
    utc_now = datetime.utcnow()
    beijing_time = utc_now + timedelta(hours=8)
    return beijing_time.strftime("%Y-%m-%d %H:%M")

def setup_gemini():
    """è®¾ç½® Gemini API"""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        log("âš ï¸ æœªè®¾ç½® GEMINI_API_KEYï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½")
        return None
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        log("âœ… Gemini API é…ç½®æˆåŠŸ")
        return model
    except Exception as e:
        log(f"âŒ Gemini API é…ç½®å¤±è´¥: {e}")
        return None

def escape_html(text: str) -> str:
    """è½¬ä¹‰ Telegram HTML æ¨¡å¼éœ€è¦çš„å­—ç¬¦"""
    if not text:
        return ""
    return (
        text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
    )

def sanitize_text(text: str) -> str:
    """ç§»é™¤æ— å…³ç¬¦å·"""
    if not text:
        return ""
    cleaned = text.replace("*", "").replace("`", "")
    cleaned = cleaned.strip('"').strip("'")
    cleaned = cleaned.replace("\\n", " ").replace("\n", " ")
    while "  " in cleaned:
        cleaned = cleaned.replace("  ", " ")
    return cleaned.strip()

def clean_ai_artifacts(text: str) -> str:
    """æ¸…ç† AI è¾“å‡ºä¸­çš„è¯´æ˜æ€§å‰ç¼€/å™ªå£°"""
    if not text:
        return ""
    cleaned = text
    patterns = [
        r"^ä»¥ä¸‹æ˜¯.*?ä¸­æ–‡ç¿»è¯‘[:ï¼š]\s*",
        r"^ç¿»è¯‘[ä¸€äºŒä¸‰]?[ï¼š:]\s*",
        r"^è§£é‡Š[ï¼š:]\s*.*$",
        r"^æˆ–[è€…]?[ï¼š:]\s*",
        r"^æ³¨[ï¼š:]\s*",
        r"^æ‚¨å¥½[ï¼Œ,\s].*$",
        r"^ä½ (å¥½|æ‚¨)æä¾›çš„æ ‡é¢˜.*?å·²ç»æ˜¯ä¸­æ–‡.*$",
        r"å·²ç»æ˜¯ä¸­æ–‡[ã€‚\.].*$",
        r"å¦‚æœæ‚¨.*è‹±æ–‡.*è¯·.*æä¾›.*åŸæ–‡.*$",
        r"^æ ‡é¢˜[ï¼š:].*$",
    ]
    for p in patterns:
        cleaned = re.sub(p, "", cleaned, flags=re.IGNORECASE|re.MULTILINE)
    cleaned = re.sub(r"ï¼ˆ[^ï¼‰]*?(è¯·åˆ é™¤|ä¸éœ€è¦|è§£é‡Š|ç¤ºä¾‹)[^ï¼‰]*?ï¼‰", "", cleaned)
    cleaned = re.sub(r"\([^)]*?(please remove|example|explanation)[^)]*?\)", "", cleaned, flags=re.IGNORECASE)
    cleaned = sanitize_text(cleaned)
    return cleaned

def summarize_text(model, title: str, content: str, fallback_chars: int = 90) -> str:
    """ç”Ÿæˆç®€çŸ­æ‘˜è¦"""
    raw = (content or "")
    if not raw and title:
        raw = title
    if model:
        try:
            prompt = (
                "ä½ æ˜¯æ–°é—»ç¼–è¾‘ã€‚ä»…è¾“å‡ºæ‘˜è¦æœ¬å¥ï¼Œä¸è¦ä»»ä½•è¯´æ˜æˆ–å‰åç¼€ï¼Œä¸è¦åŠ â€˜æ ‡é¢˜/å†…å®¹â€™ç­‰æ ‡ç­¾ã€‚"
                "ç”¨ç®€ä½“ä¸­æ–‡åœ¨60å­—å†…æ¦‚è¿°å…³é”®ä¿¡æ¯ã€‚\n\n"
                f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content}"
            )
            resp = model.generate_content(prompt)
            summary = clean_ai_artifacts((resp.text or "").strip())
            summary = summary.strip('"').strip("'")
            if len(summary) > 60:
                summary = summary[:57] + "..."
            return summary
        except Exception:
            pass
    clean = raw.replace("\n", " ").strip()
    if len(clean) > fallback_chars:
        return clean[:fallback_chars - 3] + "..."
    return clean

def translate_with_gemini(model, text):
    """ä½¿ç”¨ Gemini ç¿»è¯‘æ–‡æœ¬ï¼šè‹¥å·²æ˜¯ä¸­æ–‡åˆ™åŸæ ·è¿”å›ï¼›ç¦æ­¢è§£é‡Šæ€§å‰åç¼€"""
    if not model:
        return text

    try:
        # ç®€å•ä¸­æ–‡æ£€æµ‹ï¼šåŒ…å«CJKå­—ç¬¦åˆ™è§†ä¸ºä¸­æ–‡
        if re.search(r"[\u4e00-\u9fff]", text or ""):
            return text
        prompt = (
            "ä»…è¾“å‡ºç¿»è¯‘ç»“æœï¼ˆç®€ä½“ä¸­æ–‡ï¼‰ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€ç¤¼è²Œç”¨è¯­æˆ–å‰åç¼€ï¼›"
            "ä¸è¦å‡ºç°â€˜æ ‡é¢˜/å†…å®¹/ç¿»è¯‘ä¸º/å¦‚ä¸‹â€™ç­‰æç¤ºè¯­ï¼›ä¿æŒç®€æ´å‡†ç¡®ï¼š\n\n"
            f"{text}"
        )
        response = model.generate_content(prompt)
        translated = (response.text or "").strip()
        return clean_ai_artifacts(translated)
    except Exception as e:
        log(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
        return text

def fetch_all_news_sources(model):
    """è·å–æ‰€æœ‰æ–°é—»æº"""
    log("ğŸ“ ç”Ÿæˆç»¼åˆæ–°é—»ç®€æŠ¥...")
    
    all_news = []
    
    # 1. GNews API - é€šç”¨æ–°é—»
    gnews_key = os.getenv('GNEWS_API_KEY')
    if gnews_key:
        try:
            url = "https://gnews.io/api/v4/top-headlines"
            params = {
                'token': gnews_key,
                'lang': 'en',
                'country': 'us',
                'max': 5
            }
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, params=params, headers=headers, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                for article in data.get('articles', []):
                    all_news.append({
                        'title': article.get('title', ''),
                        'content': article.get('description', ''),
                        'source': article.get('source', {}).get('name', 'GNews'),
                        'time': article.get('publishedAt', ''),
                        'url': article.get('url', ''),
                        'type': 'general'
                    })
            log(f"âœ… GNews é€šç”¨: {len([n for n in all_news if n['type'] == 'general'])} æ¡")
        except Exception as e:
            log(f"âŒ GNews é€šç”¨å¤±è´¥: {e}")
    
    # 2. GNews API - ä¸­ç¾å…³ç³»ä¸“é¢˜
    if gnews_key:
        try:
            china_us_queries = ["China US relations", "China trade war", "China congress bill"]
            for query in china_us_queries:
                url = "https://gnews.io/api/v4/search"
                params = {
                    'token': gnews_key,
                    'q': query,
                    'lang': 'en',
                    'country': 'us',
                    'max': 2
                }
                response = requests.get(url, params=params, headers=headers, timeout=15)
                
                if response.status_code == 200:
                    data = response.json()
                    for article in data.get('articles', []):
                        all_news.append({
                            'title': article.get('title', ''),
                            'content': article.get('description', ''),
                            'source': article.get('source', {}).get('name', 'GNews'),
                            'time': article.get('publishedAt', ''),
                            'url': article.get('url', ''),
                            'type': 'china_us'
                        })
                time.sleep(1)
            log(f"âœ… GNews ä¸­ç¾å…³ç³»: {len([n for n in all_news if n['type'] == 'china_us'])} æ¡")
        except Exception as e:
            log(f"âŒ GNews ä¸­ç¾å…³ç³»å¤±è´¥: {e}")
    
    # 3. RSS æ–°é—»æºï¼ˆå¸¦å¥åº·ç›‘æ§ï¼‰
    # ä»é…ç½®è¯»å–æ¥æºï¼Œprimary ä¼˜å…ˆ
    rss_sources = []
    health_monitor = SourceHealthMonitor()
    try:
        with open('sources.json', 'r', encoding='utf-8') as f:
            cfg = json.load(f)
            # ä¼˜å…ˆæº
            for s in cfg.get('primary', []):
                if health_monitor.is_healthy(s['name']):
                    rss_sources.append((s['name'], s['url'], 1.0))  # ä¼˜å…ˆæºæƒé‡1.0
                else:
                    log(f"âš ï¸ ä¼˜å…ˆæº {s['name']} ä¸å¥åº·ï¼Œè·³è¿‡")
            # æ¬¡çº§æº
            for s in cfg.get('secondary', []):
                if health_monitor.is_healthy(s['name']):
                    rss_sources.append((s['name'], s['url'], 0.8))  # æ¬¡çº§æºæƒé‡0.8
                else:
                    log(f"âš ï¸ æ¬¡çº§æº {s['name']} ä¸å¥åº·ï¼Œè·³è¿‡")
            # å¯é€‰ç¤¾äº¤ä»£ç† RSSï¼ˆåŸºäºåˆ†ç»„ä¸æƒé‡+é•œåƒè½®æ¢ï¼‰
            social_groups = cfg.get('social_groups', {})
            nitter_mirrors = cfg.get('nitter_mirrors', ["https://nitter.net"]) 
            mirror_idx = random.randint(0, len(nitter_mirrors)-1)
            def nitter_url(handle: str) -> str:
                base = nitter_mirrors[mirror_idx % len(nitter_mirrors)].rstrip('/')
                return f"{base}/{handle}/rss"
            for group_name, group in social_groups.items():
                if group.get('enabled'):
                    group_weight = group.get('weight', 0.7)
                    for handle in group.get('accounts', []):
                        source_name = f"Twitter-{handle}"
                        if health_monitor.is_healthy(source_name):
                            rss_sources.append((source_name, nitter_url(handle), group_weight))
                        else:
                            log(f"âš ï¸ ç¤¾äº¤æº {source_name} ä¸å¥åº·ï¼Œè·³è¿‡")
    except Exception:
        # å›é€€å†…ç½®
        rss_sources = [
            ("AP News", "https://apnews.com/apf-topnews?output=rss", 1.0),
            ("Reuters World", "https://feeds.reuters.com/reuters/worldNews", 1.0),
            ("Sputnik", "https://sputniknews.com/export/rss2/archive/index.xml", 1.0),
            ("è”åˆæ—©æŠ¥", "https://www.zaobao.com.sg/rss.xml", 1.0),
            ("NPR News", "http://www.npr.org/rss/rss.php?id=1001", 0.8),
            ("HuffPost World", "https://www.huffpost.com/section/world-news/feed", 0.8),
            ("FOX News", "http://feeds.foxnews.com/foxnews/latest", 0.8),
            ("NYT Top Stories", "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml", 0.8),
            ("CNN International", "http://rss.cnn.com/rss/edition.rss", 0.8),
            ("Bloomberg", "https://feeds.bloomberg.com/markets/news.rss", 0.8)
        ]
    
    for name, url, weight in rss_sources:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            }
            
            # å¸¦é‡è¯•çš„è¯·æ±‚ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            def get_with_retry(u, headers, timeout=10, retries=3):
                delay = 1
                for attempt in range(retries):
                    try:
                        return requests.get(u, headers=headers, timeout=timeout)
                    except requests.RequestException:
                        if attempt == retries - 1:
                            raise
                        time.sleep(delay)
                        delay = min(delay * 2, 8)

            response_text = None
            # ç¤¾äº¤æºï¼šé•œåƒè½®æ¢ + RSSHub å…œåº•
            if name.startswith('Twitter-'):
                try:
                    handle = name.split('Twitter-')[-1]
                    # å°è¯• Nitter é•œåƒï¼Œæœ€å¤šå°è¯•å…¨éƒ¨é•œåƒ
                    tried = 0
                    for offset in range(len(nitter_mirrors)):
                        base = nitter_mirrors[(mirror_idx + offset) % len(nitter_mirrors)].rstrip('/')
                        social_url = f"{base}/{handle}/rss"
                        try:
                            r = get_with_retry(social_url, headers, timeout=10, retries=2)
                            if r.status_code == 200 and r.text.strip():
                                response_text = r.text
                                break
                        except Exception:
                            pass
                        tried += 1
                    # è‹¥å…¨éƒ¨ Nitter å¤±è´¥ï¼Œå°è¯• RSSHub
                    if not response_text:
                        rsshub_mirrors = cfg.get('rsshub_mirrors', ['https://rsshub.app'])
                        for rb in rsshub_mirrors:
                            rb = rb.rstrip('/')
                            # ä¼˜å…ˆä½¿ç”¨ /x/user/:id
                            social_url = f"{rb}/x/user/{handle}"
                            try:
                                r = get_with_retry(social_url, headers, timeout=12, retries=2)
                                if r.status_code == 200 and r.text.strip():
                                    response_text = r.text
                                    break
                            except Exception:
                                pass
                except Exception:
                    response_text = None
            else:
                resp = get_with_retry(url, headers, timeout=10, retries=3)
                if resp.status_code == 200:
                    response_text = resp.text

            if response_text:
                root = ET.fromstring(response_text)
                items = root.findall('.//item')
                
                for item in items[:2]:
                    title_elem = item.find('title')
                    link_elem = item.find('link')
                    description_elem = item.find('description')
                    pub_date_elem = item.find('pubDate')
                    
                    if title_elem is not None and link_elem is not None:
                        title = title_elem.text
                        link = link_elem.text
                        description = description_elem.text if description_elem is not None else ""
                        pub_date = pub_date_elem.text if pub_date_elem is not None else ""
                        
                        # å…³é”®è¯ç­›é€‰
                        keywords = ['trump', 'biden', 'president', 'election', 'china', 'russia', 'ukraine', 'israel', 'palestine', 'economy', 'market', 'trade', 'war', 'conflict', 'politics', 'government', 'congress', 'senate', 'bill', 'proposal', 'bitcoin', 'crypto', 'cryptocurrency', 'stock', 'nasdaq', 'dow', 's&p', 'sp500', 'lithium', 'nickel', 'cobalt', 'rare earth', 'graphite']
                        content = f"{title} {description}".lower()
                        
                        if any(keyword in content for keyword in keywords):
                            news_type = 'china_us' if any(word in content for word in ['china', 'chinese', 'beijing', 'taiwan', 'trade war', 'tariff', 'semiconductor', 'huawei', 'tiktok']) else 'general'
                            news_item = {
                                'title': title,
                                'content': description,
                                'source': name,
                                'time': pub_date,
                                'url': link,
                                'type': news_type,
                                'weight': weight
                            }
                            all_news.append(news_item)
                            # è®°å½•æˆåŠŸ
                            health_monitor.record_success(name, weight)
            
            time.sleep(1)
        except Exception as e:
            log(f"âŒ {name}: {e}")
            # è®°å½•å¤±è´¥
            health_monitor.record_failure(name)
    
    # 4. Gemini æœç´¢è¡¥å……
    if model:
        search_queries = [
            "Trump latest news today",
            "US politics today",
            "China US relations latest",
            "International news key developments"
        ]
        
        for query in search_queries:
            try:
                prompt = f"è¯·æœç´¢å…³äº'{query}'çš„æœ€æ–°æ–°é—»ï¼Œæä¾›2æ¡æœ€é‡è¦çš„æ–°é—»ï¼Œæ ¼å¼ä¸ºJSONï¼š[{{\"title\": \"æ ‡é¢˜\", \"content\": \"å†…å®¹\", \"source\": \"æ¥æº\", \"time\": \"æ—¶é—´\", \"url\": \"é“¾æ¥\"}}]"
                response = model.generate_content(prompt)
                result_text = response.text
                
                try:
                    start_idx = result_text.find('[')
                    end_idx = result_text.rfind(']') + 1
                    if start_idx != -1 and end_idx != -1:
                        json_text = result_text[start_idx:end_idx]
                        news_data = json.loads(json_text)
                        for news in news_data:
                            news['type'] = 'china_us' if 'china' in query.lower() else 'general'
                        all_news.extend(news_data)
                except:
                    pass
                
                time.sleep(2)
            except Exception as e:
                log(f"âŒ Gemini æœç´¢å¤±è´¥: {e}")
    
    # å»é‡å’Œæ’åº
    seen_titles = set()
    unique_news = []
    for news in all_news:
        title = news.get('title', '').lower()
        if title not in seen_titles and len(title) > 10:
            seen_titles.add(title)
            unique_news.append(news)
    
    unique_news.sort(key=lambda x: x.get('time', ''), reverse=True)
    
    log(f"ğŸ“Š æ€»å…±æ”¶é›† {len(unique_news)} æ¡æ–°é—»")
    return unique_news

def send_telegram_message(bot_token, chat_id, text):
    """å‘é€ Telegram æ¶ˆæ¯"""
    log("ğŸ“¤ å‘é€ Telegram æ¶ˆæ¯...")
    
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {
        'chat_id': chat_id,
        'text': text,
        'parse_mode': 'Markdown',
        'disable_web_page_preview': True
    }
    
    try:
        log(f"æ¶ˆæ¯é•¿åº¦: {len(text)} å­—ç¬¦")
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('ok'):
                log("âœ… Telegram æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                log(f"âŒ Telegram API é”™è¯¯: {result.get('description')}")
                return False
        else:
            log(f"âŒ HTTP é”™è¯¯: {response.status_code}")
            return False
            
    except Exception as e:
        log(f"âŒ å‘é€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    log("ğŸš€ ç»¼åˆç‰ˆæ–°é—» Telegram Bot å¯åŠ¨")
    log("=" * 50)
    
    try:
        # 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
        log("ğŸ“‹ æ£€æŸ¥ç¯å¢ƒå˜é‡...")
        bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        chat_id = os.getenv('CHAT_ID')
        
        if not bot_token or not chat_id:
            log("âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        log("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
        
        # 2. è®¾ç½® Gemini API
        model = setup_gemini()
        
        # 3. è·å–æ‰€æœ‰æ–°é—»
        all_news = fetch_all_news_sources(model)
        
        # 4. åˆå§‹åŒ–ç¼“å­˜å’Œå¥åº·ç›‘æ§
        news_cache = NewsCache()
        health_monitor = SourceHealthMonitor()
        
        # æ¸…ç†æ—§ç¼“å­˜
        news_cache.cleanup_old_news(24)
        
        # 5. åˆ†ç±»æ–°é—»ï¼ˆæ–°å¢æ ‡ç­¾ï¼šä¿„ä¹Œå†²çªã€å…³é”®çŸ¿äº§ã€è™šæ‹Ÿè´§å¸ä¸å…¨çƒè‚¡å¸‚ï¼‰
        lower = lambda s: (s or '').lower()
        trump_news = [n for n in all_news if 'trump' in lower(n.get('title')) or 'trump' in lower(n.get('content'))]
        china_us_news = [n for n in all_news if (n.get('type') == 'china_us' or 'china' in lower(n.get('title')) or 'china' in lower(n.get('content'))) and n not in trump_news]
        ru_ua_news = [n for n in all_news if any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['ukraine','zelensky','russia','kremlin','putin','donbas','crimea'])]
        minerals_news = [n for n in all_news if any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['lithium','nickel','cobalt','rare earth','graphite','copper','critical mineral','mining','battery metal'])]
        crypto_markets_news = [n for n in all_news if any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['bitcoin','crypto','cryptocurrency','ethereum','nasdaq','dow','s&p','sp500','stocks','markets','equities','fed'])]
        other_news = [n for n in all_news if n not in trump_news and n not in china_us_news and n not in ru_ua_news and n not in minerals_news and n not in crypto_markets_news]
        
        # æƒé‡æ’åºå‡½æ•°ï¼šæŒ‰æƒé‡Ã—æ—¶é—´æ–°é²œåº¦æ’åº
        def weight_sort_key(item):
            weight = item.get('weight', 1.0)
            time_str = item.get('time', '')
            # ç®€å•çš„æ—¶é—´æ–°é²œåº¦è¯„åˆ†ï¼ˆè¶Šæ–°è¶Šé«˜ï¼‰
            time_score = 1.0
            if 'today' in time_str.lower() or 'just' in time_str.lower():
                time_score = 1.0
            elif 'hour' in time_str.lower():
                time_score = 0.9
            elif 'minute' in time_str.lower():
                time_score = 0.95
            else:
                time_score = 0.8
            return weight * time_score

        # å›å¡«å·¥å…·ï¼šå°†åˆ—è¡¨è¡¥é½åˆ°ç›®æ ‡æ¡æ•°ï¼Œä¼˜å…ˆåŒä¸»é¢˜å®½åŒ¹é…ï¼Œå†æ¬¡ä¸ºæ€»ä½“æŒ‰æ—¶é—´æœ€æ–°
        def unique_key(item):
            return (item.get('title','').strip().lower())

        def fill_to_count(primary, target, wide_filter=None):
            selected_keys = {unique_key(x) for x in primary}
            result = list(primary)
            # å€™é€‰æ± ï¼šæŒ‰æƒé‡Ã—æ—¶é—´æ–°é²œåº¦æ’åº
            pool = sorted(all_news, key=weight_sort_key, reverse=True)
            # ç¬¬ä¸€è½®ï¼šæŒ‰å®½åŒ¹é…æ¡ä»¶è¡¥é½
            if wide_filter:
                for item in pool:
                    if len(result) >= target:
                        break
                    if unique_key(item) in selected_keys:
                        continue
                    if wide_filter(item):
                        result.append(item)
                        selected_keys.add(unique_key(item))
            # ç¬¬äºŒè½®ï¼šä»»æ„æœ€æ–°è¡¥é½
            for item in pool:
                if len(result) >= target:
                    break
                if unique_key(item) in selected_keys:
                    continue
                result.append(item)
                selected_keys.add(unique_key(item))
            # æœ€ç»ˆæŒ‰æƒé‡æ’åº
            final_result = sorted(result, key=weight_sort_key, reverse=True)[:target]
            
            # ç¼“å­˜æ–°é—»åˆ°æ•°æ®åº“
            for item in final_result:
                category = "trump" if wide_filter and "trump" in str(wide_filter).lower() else "general"
                news_cache.add_news(item, category, item.get('weight', 1.0))
            
            return final_result

        def fill_to_count_with_cache(primary, category, target, wide_filter=None):
            """å¸¦ç¼“å­˜çš„è¡¥é½å‡½æ•°"""
            selected_keys = {unique_key(x) for x in primary}
            result = list(primary)
            # å€™é€‰æ± ï¼šæŒ‰æƒé‡Ã—æ—¶é—´æ–°é²œåº¦æ’åº
            pool = sorted(all_news, key=weight_sort_key, reverse=True)
            # ç¬¬ä¸€è½®ï¼šæŒ‰å®½åŒ¹é…æ¡ä»¶è¡¥é½
            if wide_filter:
                for item in pool:
                    if len(result) >= target:
                        break
                    if unique_key(item) in selected_keys:
                        continue
                    if wide_filter(item):
                        result.append(item)
                        selected_keys.add(unique_key(item))
            # ç¬¬äºŒè½®ï¼šä»»æ„æœ€æ–°è¡¥é½
            for item in pool:
                if len(result) >= target:
                    break
                if unique_key(item) in selected_keys:
                    continue
                result.append(item)
                selected_keys.add(unique_key(item))
            # æœ€ç»ˆæŒ‰æƒé‡æ’åº
            final_result = sorted(result, key=weight_sort_key, reverse=True)[:target]
            
            # ç¼“å­˜æ–°é—»åˆ°æ•°æ®åº“
            for item in final_result:
                news_cache.add_news(item, category, item.get('weight', 1.0))
            
            return final_result

        # å®šä¹‰å„ä¸»é¢˜çš„å®½åŒ¹é…å‡½æ•°å¹¶è¡¥é½åˆ°10æ¡
        trump_news = fill_to_count_with_cache(
            trump_news, "trump", 10,
            wide_filter=lambda n: 'white house' in lower(n.get('content')) or 'president' in lower(n.get('content'))
        )
        china_us_news = fill_to_count_with_cache(
            china_us_news, "china_us", 10,
            wide_filter=lambda n: any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['china','beijing','taiwan','tariff','semiconductor','huawei','tiktok','congress','bipartisan'])
        )
        ru_ua_news = fill_to_count_with_cache(
            ru_ua_news, "ru_ua", 10,
            wide_filter=lambda n: any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['ukraine','russia','kremlin','moscow','kyiv','nato'])
        )
        minerals_news = fill_to_count_with_cache(
            minerals_news, "minerals", 10,
            wide_filter=lambda n: any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['lithium','nickel','cobalt','rare earth','graphite','copper','mining','battery'])
        )
        crypto_markets_news = fill_to_count_with_cache(
            crypto_markets_news, "crypto_markets", 10,
            wide_filter=lambda n: any(k in lower(n.get('title')) or k in lower(n.get('content')) for k in ['bitcoin','crypto','ethereum','nasdaq','dow','s&p','sp500','stocks','market'])
        )
        
        # 5. æ ¼å¼åŒ–æ¶ˆæ¯ï¼ˆéµå¾ªç”¨æˆ·æä¾›çš„å‡ºç‰ˆæ ¼å¼ï¼‰
        timestamp = get_beijing_timestamp()
        
        # åª’ä½“ä¸­æ–‡åæ˜ å°„
        def source_cn(name: str) -> str:
            n = (name or '').strip()
            mapping = {
                'CNN': 'æœ‰çº¿ç”µè§†æ–°é—»ç½‘ CNN',
                'CNN International': 'æœ‰çº¿ç”µè§†æ–°é—»ç½‘ CNN',
                'The New York Times': 'çº½çº¦æ—¶æŠ¥ NYT',
                'NYT Top Stories': 'çº½çº¦æ—¶æŠ¥ NYT',
                'Reuters': 'è·¯é€ç¤¾ Reuters',
                'Reuters World': 'è·¯é€ç¤¾ Reuters',
                'Bloomberg': 'å½­åšç¤¾ Bloomberg',
                'Fox News': 'ç¦å…‹æ–¯æ–°é—» FOX',
                'FOX News': 'ç¦å…‹æ–¯æ–°é—» FOX',
                'Washington Post': 'åç››é¡¿é‚®æŠ¥ WP',
                'The Washington Post': 'åç››é¡¿é‚®æŠ¥ WP',
                'NPR': 'ç¾å›½å›½å®¶å…¬å…±ç”µå° NPR',
                'NPR News': 'ç¾å›½å›½å®¶å…¬å…±ç”µå° NPR',
                'Financial Times': 'é‡‘èæ—¶æŠ¥ FT',
                'The Wall Street Journal': 'åå°”è¡—æ—¥æŠ¥ WSJ',
                'Wall Street Journal': 'åå°”è¡—æ—¥æŠ¥ WSJ',
                'HuffPost': 'èµ«èŠ¬é¡¿é‚®æŠ¥ HuffPost',
                'AP News': 'ç¾è”ç¤¾ AP',
                'Associated Press': 'ç¾è”ç¤¾ AP',
                'è”åˆæ—©æŠ¥': 'è”åˆæ—©æŠ¥ LHZB',
                'Sputnik': 'ä¿„ç½—æ–¯å«æ˜Ÿé€šè®¯ç¤¾ Sputnik',
            }
            return mapping.get(n, n or 'æœªçŸ¥æ¥æº')

        def extract_time_iso(time_str: str) -> str:
            if not time_str:
                return ''
            t = time_str.replace('T', ' ').replace('Z', '').strip()
            return t[:16]

        def extract_entities(text: str) -> str:
            if not text:
                return ''
            keywords = ['ç‰¹æœ—æ™®', 'æ‹œç™»', 'å“ˆé‡Œæ–¯', 'å›½ä¼š', 'å‚è®®é™¢', 'ä¼—è®®é™¢', 'ç™½å®«', 'ç¾å›½', 'ä¸­å›½', 'åŒ—äº¬', 'å°æ¹¾', 'åä¸º', 'TikTok', 'è‹±ä¼Ÿè¾¾', 'ç¾è”å‚¨']
            found = []
            for k in keywords:
                if k in text:
                    found.append(k)
            en_keywords = ['Trump', 'Biden', 'Congress', 'Senate', 'House', 'White House', 'China', 'Taiwan', 'Huawei', 'TikTok', 'Nvidia', 'Federal Reserve']
            for k in en_keywords:
                if k.lower() in text.lower():
                    found.append(k)
            uniq = []
            for x in found:
                if x not in uniq:
                    uniq.append(x)
            return 'ï¼›'.join(uniq[:4])

        def format_source_link(name: str, url: str) -> str:
            txt = source_cn(name)
            if url:
                return f'[{txt}]({url})'
            return txt

        if all_news:
            lines = []
            lines.append('æ¯æ—¥ç»¼åˆè¦é—»ç®€æŠ¥')
            lines.append(f'æ›´æ–°æ—¶é—´ï¼š{timestamp} (UTC+8)')
            lines.append(f'ä»Šæ—¥æ”¶å½•ï¼š{len(all_news)}æ¡é‡ç‚¹æ–°é—»')
            
            # æºå¥åº·çŠ¶æ€ç»Ÿè®¡
            healthy_sources = 0
            total_sources = 0
            try:
                conn = sqlite3.connect("sources_health.db")
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM source_health WHERE is_healthy = 1")
                healthy_sources = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM source_health")
                total_sources = cursor.fetchone()[0]
                conn.close()
            except:
                pass
            
            if total_sources > 0:
                health_rate = (healthy_sources / total_sources) * 100
                lines.append(f'æºå¥åº·çŠ¶æ€ï¼š{healthy_sources}/{total_sources} ({health_rate:.1f}%)')
            
            lines.append('')
            lines.append('---')
            lines.append('')

            # ç‰¹æœ—æ™®æ€»ç»ŸåŠ¨æ€
            lines.append('## ä¸€ã€ç‰¹æœ—æ™®æ€»ç»ŸåŠ¨æ€')
            lines.append('')
            for idx, news in enumerate(trump_news[:3], 1):
                title = clean_ai_artifacts(sanitize_text(translate_with_gemini(model, news.get('title','')) if model else news.get('title','')))
                summary_base = clean_ai_artifacts(sanitize_text(summarize_text(model, news.get('title',''), news.get('content',''))))
                time_part = extract_time_iso(news.get('time',''))
                entity_part = extract_entities(title + ' ' + (news.get('content','') or ''))
                summary = summary_base
                if time_part:
                    summary = f"æ—¶é—´ï¼š{time_part}ï¼›" + summary
                if entity_part:
                    summary = f"äººç‰©/æœºæ„ï¼š{entity_part}ï¼›" + summary
                src = format_source_link(news.get('source','æœªçŸ¥æ¥æº'), news.get('url',''))
                if len(title) > 80:
                    title = title[:77] + '...'
                lines.append(f'{idx}. **{title}**')
                if summary:
                    lines.append(f'æ‘˜è¦ï¼š{summary}')
                lines.append(f'æ¥æºï¼š{src}')
                lines.append('')

            lines.append('---')
            lines.append('')

            # ä¸­ç¾å…³ç³»ä¸“é¢˜
            lines.append('## äºŒã€ä¸­ç¾å…³ç³»ä¸“é¢˜')
            lines.append('')
            for idx, news in enumerate(china_us_news[:10], 1):
                title = clean_ai_artifacts(sanitize_text(translate_with_gemini(model, news.get('title','')) if model else news.get('title','')))
                summary_base = clean_ai_artifacts(sanitize_text(summarize_text(model, news.get('title',''), news.get('content',''))))
                time_part = extract_time_iso(news.get('time',''))
                entity_part = extract_entities(title + ' ' + (news.get('content','') or ''))
                summary = summary_base
                if time_part:
                    summary = f"æ—¶é—´ï¼š{time_part}ï¼›" + summary
                if entity_part:
                    summary = f"äººç‰©/æœºæ„ï¼š{entity_part}ï¼›" + summary
                src = format_source_link(news.get('source','æœªçŸ¥æ¥æº'), news.get('url',''))
                if len(title) > 80:
                    title = title[:77] + '...'
                lines.append(f'{idx}. **{title}**')
                if summary:
                    lines.append(f'æ‘˜è¦ï¼š{summary}')
                lines.append(f'æ¥æºï¼š{src}')
                lines.append('')

            lines.append('---')
            lines.append('')

            # ä¿„ä¹Œå†²çªåŠ¨æ€
            lines.append('## ä¸‰ã€ä¿„ä¹Œå†²çªåŠ¨æ€')
            lines.append('')
            for idx, news in enumerate(ru_ua_news[:10], 1):
                title = clean_ai_artifacts(sanitize_text(translate_with_gemini(model, news.get('title','')) if model else news.get('title','')))
                summary_base = clean_ai_artifacts(sanitize_text(summarize_text(model, news.get('title',''), news.get('content',''))))
                time_part = extract_time_iso(news.get('time',''))
                entity_part = extract_entities(title + ' ' + (news.get('content','') or ''))
                summary = summary_base
                if time_part:
                    summary = f"æ—¶é—´ï¼š{time_part}ï¼›" + summary
                if entity_part:
                    summary = f"äººç‰©/æœºæ„ï¼š{entity_part}ï¼›" + summary
                src = format_source_link(news.get('source','æœªçŸ¥æ¥æº'), news.get('url',''))
                if len(title) > 80:
                    title = title[:77] + '...'
                lines.append(f'{idx}. **{title}**')
                if summary:
                    lines.append(f'æ‘˜è¦ï¼š{summary}')
                lines.append(f'æ¥æºï¼š{src}')
                lines.append('')

            lines.append('---')
            lines.append('')
            # å…³é”®çŸ¿äº§åˆä½œ
            lines.append('## å››ã€å…³é”®çŸ¿äº§åˆä½œ')
            lines.append('')
            for idx, news in enumerate(minerals_news[:10], 1):
                title = clean_ai_artifacts(sanitize_text(translate_with_gemini(model, news.get('title','')) if model else news.get('title','')))
                summary_base = clean_ai_artifacts(sanitize_text(summarize_text(model, news.get('title',''), news.get('content',''))))
                time_part = extract_time_iso(news.get('time',''))
                entity_part = extract_entities(title + ' ' + (news.get('content','') or ''))
                summary = summary_base
                if time_part:
                    summary = f"æ—¶é—´ï¼š{time_part}ï¼›" + summary
                if entity_part:
                    summary = f"äººç‰©/æœºæ„ï¼š{entity_part}ï¼›" + summary
                src = format_source_link(news.get('source','æœªçŸ¥æ¥æº'), news.get('url',''))
                if len(title) > 80:
                    title = title[:77] + '...'
                lines.append(f'{idx}. **{title}**')
                if summary:
                    lines.append(f'æ‘˜è¦ï¼š{summary}')
                lines.append(f'æ¥æºï¼š{src}')
                lines.append('')

            lines.append('---')
            lines.append('')

            # è™šæ‹Ÿè´§å¸å’Œå…¨çƒè‚¡å¸‚åŠ¨æ€
            lines.append('## äº”ã€è™šæ‹Ÿè´§å¸å’Œå…¨çƒè‚¡å¸‚åŠ¨æ€')
            lines.append('')
            for idx, news in enumerate(crypto_markets_news[:10], 1):
                title = clean_ai_artifacts(sanitize_text(translate_with_gemini(model, news.get('title','')) if model else news.get('title','')))
                summary_base = clean_ai_artifacts(sanitize_text(summarize_text(model, news.get('title',''), news.get('content',''))))
                time_part = extract_time_iso(news.get('time',''))
                entity_part = extract_entities(title + ' ' + (news.get('content','') or ''))
                summary = summary_base
                if time_part:
                    summary = f"æ—¶é—´ï¼š{time_part}ï¼›" + summary
                if entity_part:
                    summary = f"äººç‰©/æœºæ„ï¼š{entity_part}ï¼›" + summary
                src = format_source_link(news.get('source','æœªçŸ¥æ¥æº'), news.get('url',''))
                if len(title) > 80:
                    title = title[:77] + '...'
                lines.append(f'{idx}. **{title}**')
                if summary:
                    lines.append(f'æ‘˜è¦ï¼š{summary}')
                lines.append(f'æ¥æºï¼š{src}')
                lines.append('')

            lines.append('---')
            lines.append('')
            lines.append('æ¥æºï¼šå¤šå®¶æƒå¨åª’ä½“ + AI æ‘˜è¦')

            message_text = "\n".join(lines)
            if len(message_text) > 4000:
                message_text = message_text[:3997] + '...'
            message = message_text
        else:
            message = f"""ğŸŒ æ¯æ—¥ç»¼åˆè¦é—»ç®€æŠ¥

ğŸ“… æ›´æ–°æ—¶é—´: {timestamp} (UTC+8)

âš ï¸ ä»Šæ—¥æ–°é—»æºæš‚æ—¶ä¸å¯ç”¨
ğŸ¤– è¯·ç¨åæŸ¥çœ‹æ–°é—»ç½‘ç«™è·å–æœ€æ–°èµ„è®¯

ğŸ’¡ æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ¢å¤æœåŠ¡..."""
        
        # 6. å‘é€æ¶ˆæ¯
        success = send_telegram_message(bot_token, chat_id, message)
        
        if success:
            log("ğŸ‰ ä»»åŠ¡å®Œæˆ! æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ° Telegram")
        else:
            log("âŒ ä»»åŠ¡å¤±è´¥! æ¶ˆæ¯å‘é€å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        log(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
